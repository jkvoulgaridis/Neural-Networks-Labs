# -*- coding: utf-8 -*-
"""Άσκηση2-final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qZC-xILUiCx07YUrJ3i_m2XBKnzK0TBP

# Δεύτερη Άσκηση νευρωνικών

Ομάδα 49 <br>
Γιάννης Βουλγαρίδης, 03116151 <br>
Μαρία-Ιωάννα Σωτηρίου, 03116090 <br>
Ευάγγελος Ζάχος, 03116015 <br>

# Εφαρμογή 1-Σύστημα Προτάσεων ταινιών
"""

!pip install --upgrade somoclu

"""Σύμφωνα με τις οδηγίες της εκφώνησης το seed ορίζεται ίσο με 49."""

#import data
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
from nltk import word_tokenize          
from nltk.stem import PorterStemmer 
import nltk
class LemmaTokenizer(object):
    def __init__(self):
        self.wnl = WordNetLemmatizer()
    def __call__(self, articles):
        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]

dataset_url = "https://drive.google.com/uc?export=download&id=1PdkVDENX12tQliCk_HtUnAUbfxXvnWuG"
df_data_1 = pd.read_csv(dataset_url, sep='\t',  header=None, quoting=3, error_bad_lines=False)

print(df_data_1.shape)
team_seed_number = 49 
movie_seeds_url = "https://drive.google.com/uc?export=download&id=1EA_pUIgK5Ub3kEzFbFl8wSRqAV6feHqD"
df_data_2 = pd.read_csv(movie_seeds_url, header=None, error_bad_lines=False)
print(df_data_2.shape)

my_index = df_data_2.iloc[team_seed_number,:].values

titles = df_data_1.iloc[:, [2]].values[my_index] # movie titles (string)
categories = df_data_1.iloc[:, [3]].values[my_index] # movie categories (string)
bins = df_data_1.iloc[:, [4]]
catbins = bins[4].str.split(',', expand=True).values.astype(np.float)[my_index] # movie categories in binary form (1 feature per category)
summaries =  df_data_1.iloc[:, [5]].values[my_index] # movie summaries (string)
corpus = summaries[:,0].tolist() # list form of summaries

print(corpus[:3])

#corpus to tdif 

from sklearn.feature_extraction.text import TfidfVectorizer
corpusTfIdf = np.zeros(5000)

"""Στο επόμενο κελί κώδικα υλοποιούνται δύο συναρτήσεις. Η printMoviesData(target_movie) είναι μια βοηθητική συνάρτηση η οποία τυπώνει με τρόπο οργανωμένο τα στοιχεία της ταινίας με index target_movie. Επίσης υλοποιείται η συνάρτηση content_reccomender η οποία δέχεται δύο ορίσματα, το id μιας ταινίας και έναν αριθμό που ορίζει πόσες προτάσεις θα επιστραφούν. """

import scipy as sp

def printMovieData(target_movie):
  id = target_movie
  titleId = titles[id][0]
  catId = categories[id][0]
  summaryId = ' '.join(summaries[id])
  print(f'target movie Id : {id}')
  print(f'target movie title : "{titleId}"')
  print(f'target movie categories {catId}')
  print(f'target movie summary : ')
  summaryId = summaryId.split(' ')
  #print(summaryId)
  s = ''
  for i in range(len(summaryId)):
    if len(s) + len(summaryId[i]) < 80:
      s += summaryId[i] + ' '
    else:
      print(s)
      s = summaryId[i] + ' '
  print('{:100s}'.format(s))
    
def content_reccomender(target_movie , max_reccomendations):
  id = target_movie
  titleId = titles[id][0]
  catId = categories[id][0]
  summaryId_tfidf = corpusTfIdf[id]
  printMovieData(target_movie)
  dists = []
  print ('********************************************************************************')
  print('\t\t\t\tRECCOMENDATIONS')
  print ('********************************************************************************')
  for i in range(titles.shape[0]):
    if i == target_movie:
      continue
    else:
      dist = sp.spatial.distance.cosine(corpusTfIdf[i] , summaryId_tfidf)
      dists.append((dist,i))
  
  dists.sort()
  #print(dists)
  dists = dists[:max_reccomendations] 
  for x  in dists:
    printMovieData(x[1])
    print('*******************************************************************************')

"""Στο επόμενο κελί κώδικα κατασκευάζεται ένας TfidfVectorizer χωρίς ορίσματα, με default τιμές. Στην συνέχεια, εφαρμόζουμε τον μετασχηματισμό του vectorizer στο corpus και σημηουργούμε το corpusTfIdf το οποίο χρησιμοποιεί η συνάρτηση content_reccomender. """

'''
baseline recocmmnender
'''
vectorizer = TfidfVectorizer()
corpusTfIdf=vectorizer.fit_transform(corpus).toarray()
joblib.dump(corpusTfIdf , 'baseline_TfidfCorpus.pkl')
#corpusTfIdf = joblib.load('baseline_TfidfCorpus.pkl')
print(corpusTfIdf.shape)
content_reccomender(2745, 2)

"""## Βελτιστοποίηση Vectorizer - stop words

Οι παραπάνω προτάσεις δεν είναι κοντά σε αυτά που θα θέλαμε αλλά παρόλα αυτά βρέθηκαν σημασιολογικά κοντά βάσει της απόστασης συνημιτόνου. Ο λόγος είναι ότι κάποιες λέξεις είναι πολύ κοινές και δεν δίνουν πληροφορία (πχ. 'a', 'the', 'in') στα αγγλικά. Λέξεις σαν αυτές θα θέλαμε να  μην λαμβάνονται υπόψιν κατά την επεξεργασία. Αυτό γίνεται θέτοντας στην παράμετρο stop_words του TfIdfVectorizer έναν πίνακα με όλες τις λέξεις που θα θέλαμε να αφαιρούνται από την επεξεργασία. Στο σύνδεσμο https://www.rypeapp.com/most-common-english-words/ θα βρείτε ένα άρθρο το οποίο απαριθμεί τις 100 συχνότερες λέξεις της αγγλικής γλώσσας. Επεξεργαστήκαμε το string αυτό για να κρατήσουμε τις λέξεις σε μορφή πίνακα και στην συνέχεια προσθέσαμε και κάποιες λέξεις οι οποίες αναμένεται να εμφανίζονται συχνά, λέξεις όπως 'movie', 'film', 'actor', 'plot'. Στο παρακάτω κελί φαίνεται αυτή η επεξεργασία των stopwords και στην συνέχεια η κατασκευή ενός καλύτερου TidfVectorzer και η εφαρμογή του στο corpus ώστε να προκύψει ένα νέο TfIdfcorpus. Επίσης, παρατηρήσαμε ότι σε πολλές ταινίες προκύπτουν προτάσεις μόνο και μόνο επειδή αναφέρεται ένα όνομα. Για παράδειγμα αν στην περίληψη της ταινίας στόχου υπάρχει η πρόταση "Η Άννα είναι μια νεαρή δασκάλα που έχει σχέση με το Χάρη", είναι πολύ πιθανό στις προτάσεις να εμφανιστούν ταινίες με μη σχετικό περιεχόμενο οι οποίες έχουν έναν χαρακτήρα με όνομα "Άννα" ή "Χάρης". Για τον λόγο αυτό βασιστήκαμε σε αυτό  άρθρο ( https://www.ef.com/in/english-resources/english-names/ ) που αναφέρει τα πιο συχνά ονόματα στα Αγγλικά και τα προσθέσαμε και αυτά ως stop Words.
"""

'''
reference for most used words: https://www.rypeapp.com/most-common-english-words/
'''
import itertools as it

string = \
'''
1. the	21. at	41. there	61. some	81. my
2. of	22. be	42. use	62. her	82. than
3. and	23. this	43. an	63. would	83. first
4. a	24. have	44. each	64. make	84. water
5. to	25. from	45. which	65. like	85. been
6. in	26. or	46. she	66. him	86. call
7. is	27. one	47. do	67. into	87. who
8. you	28. had	48. how	68. time	88. oil
9. that	29. by	49. their	69. has	89. its
10. it	30. word	50. if	70. look	90. now
11. he	31. but	51. will	71. two	91. find
12. was	32. not	52. up	72. more	92. long
13. for	33. what	53. other	73. write	93. down
14. on	34. all	54. about	74. go	94. day
15. are	35. were	55. out	75. see	95. did
16. as	36. we	56. many	76. number	96. get
17. with	37. when	57. then	77. no	97. come
18. his	38. your	58. them	78. way	98. made
19. they	39. can	59. these	79. could	99. may
20. I	40. said	60. so	80. people	100. part
'''

names = \
[
  'oliver', 'jack', 'harry',\
  'jacob', 'charlie', 'thomas',\
  'george', 'oscar', 'james', \
  'william' , 'amelia', 'olivia', \
  'isla', 'emily', 'poppy', 'ava' , \
  'isabella', 'jessica', 'lily', 'sophie',\
  'ann', 'anna', 'margot', 'pauline'  ,\
  'kate', 'alex' \
]

stopWords=string.replace('.','').replace('\t',' ').replace('\n', ' ').split(' ')
stopWords=[stopWords[i] for i in range(2,len(stopWords)-1,2)]

stopWords.append('movie')
stopWords.append('film')
stopWords.append('plot')
stopWords.append('actor')
stopWords = list(it.chain(stopWords, names))
joblib.dump(stopWords,'stopWords.pkl')

print(stopWords)

"""## Βελτιστοποίηση Vectorizer- Προεπεξεργασία corpus

Στο επόμενο στάδιο, περνάμε το corpus από κάποιους μετασχηματισμούς προκειμένου να αναδείξουμε κάποιες ακόμα σχέσεις. 
* Αρχικά, μετατρέπουμε όλα τα γράμματα των λέξεων σε πεζά. Με τον τρόπο αυτό θα μπορεί να καταλάβει ότι η λέξη στην αρχή της πρότασης μπορεί να είναι ίδια με μια στην μέση της. Το τίμημα αυτής της μετατροπής είναι ότι χάνουμε την πληροφορία αν κάτι είναι τοπονύμιο, π.χ. το 'New York' θα γίνει 'new york' αλλά αυτό δεν μας πειράζει αφού και έτσι θα βρεί ταινίες που διαδρματίζονται στην new york. 

* Χρησιμοποιούμε έναν stemmer για να συσχετίσουμε τις λέξεις στις διάφορες κλίσεις τους, πχ να μπορεί να συσχετίσει το 'having played chess he left' και το 'play chess with me'. 

* Στην συνέχεια αφαιρέθηκαν τα stopWords που φτιάξαμε παραπάνω εμπλουτίζοντας τα και με αυτά άπό το nltk πακέτο.

* Επίσης, στον TfIdfVectorizer χρησιμοποιήσαμε max_df = 0.6 και min_df = 20 για να αγνοήσουμε τις 'ακραίες' λέξεις και προς τις δύο κατευθύνσεις (και τις συχνές και τις σπάνιες).
"""

nltk.download('stopwords')
stopWords = set(list(nltk.corpus.stopwords.words('english'))  + list((stopWords)) )
corpus2 = [cor.lower() for cor in corpus]

stemmer = nltk.stem.PorterStemmer()

corpus3 = [[w for w in a.split() if w not in stopWords] for a  in corpus2]

print(corpus[1])
print(corpus3[1])

corpus4 = [" ".join([stemmer.stem(word) for word in article]) for article in corpus3]
print(corpus4[1])

'''
new - imroved vectorizer
'''
vectorizer = TfidfVectorizer(max_df=0.6 , min_df= 20)
corpusTfIdf=vectorizer.fit_transform(corpus4).toarray()
joblib.dump(corpusTfIdf , 'corpus_tfidf_transform.pkl')
print(corpusTfIdf.shape)
content_reccomender(2745,6)

"""Τα αποτελέσματα της προηγούμενης εκτέλεσης είναι πιο ενθαρυντικά. Οι προτάσεις έχουν μια συνοχή. Οι βασικές λέξεις κλειδία που εμφανίζονται τόσο στην ταινία-στόχο όσο και στις προτάσεις είναι οι λέξεις 'cult','religion', 'cop', 'investigates'. στο επόμενο βήμα θα προσπαθήσουμε να εισάγουμε και δύο ακόμα παραμέτρους για να βελτιώσουμε το αποτέλεσμα, το max_df και το min_df.  Το max_df αγνοεί τις λέξεις οι οποίες εμφανίζονται πιο συχνά από ένα δεδομένο threshold και το min_df αγνοεί τις υπερβολικά σπάνιες λέξεις.  
<br>
Μετά από δοκιμές χρησιμοποιήσαμε max_df=0.7 και min_df=3.

## Παραδείγματα λειτουργίας

Για το συγκεκριμένο dataset (με seed=49) κάποιες ταινίες που δίνουν καλές προτάσεις (δοκιμάστηκαν 6 προτάσεις ανά ID) είναι :
* id=2745 : Η ταινία με την οποία έγινε και η βελτιστοποίηση του TfIdfVectorizer. Οι λέξεις κλειδιά οι οποίες συνδέουν τον στόχο με τις προτάσεις είναι οι λέξεις 'cult', 'religion', 'demon'. 

* id=936 : Οι ταινία αυτή είναι σχετίζεται με την υποχρεωτική στρατιωτική θητεία 3 φίλων στην Αμερική του μέλλοντος. Οι προτάσεις ήταν όλες σχετικές με πόλεμο, μεταπολεμικά τραύματα κλπ. οι λέξεις κλειδιά ο οποίες εμφανίζοταν **και** στην περίληψη της ταινίας στόχου και στην περίληψη των προτάσεων είναι 'war', 'pre war', 'terror', 'America', 'military', 'attack', 'therapist', 'soldier'. 

* id=6 : Η ταινία αυτή ιστορική μεσαιωνική. Η περίληψη περιλαμβάνει τις λέξεις κλειδία 'king', 'queen', 'prince', 'throne', 'rule', 'army', 'commander'. Καθεμιά από τις προτάσεις περιλαμβάνουν πολλές από τις παραπάνω λέξεις.

* id=1115: Η ταινία αυτή περιέχει τις λέξεις κλειδιά: 'husband' , 'wife'. Όλες οι ταινίες-προτάσεις αφορούν την ζωή καποιου ζευγαρού.  

* id=4999: Η ταινία συνδέεται με τις προτάσεις με τις λέξεις κλειδιά 'robot', 'malfunctions' , 'fight', 'life'. Οι ταινίες εδώ έχουν δύο θεματικές. Κάποιες προτάσεις (οι πρώτες) αφορούν μια μελλοντική ρομποτική δυστοπία όπου τα ρομπότ ελέγχουν πολλά καίρια συστήματα και η πλοκή ξετυλίγεται γύρω από καποια δυσλειτουργίας τους και η δεύτερη θεματική είναι ταινίες αθλημάτων box κλπ. Οι πρώτες ταινίες περιέχουν όλες τις παραπάνω λέξεις και οι ταινίες της δεύτερης θεματικής δεν περιέχουν την λέξη robot, αλλά είναι ωστόσο συναφείς βάσει της περίληψης τους.   

* id=280: Η ταινία περιέχει τις λέξεις κλειδιά 'high shcool', 'teacher' , 'murder', 'crime', 'kill'. Οι προτάσεις που προκύπτουν έχουν στις περιλήψεις τους αυτές τις λέξεις και έχουν κοινό ότι διαδραμτίζονται σε σχολεία, με κάποιον καθηγητή να συμμετέχει και κάποιο έγκλημα να συμβαίνει. 

* id=340: η ταινία συσχετίζεται μς τις παραγόμενες προτάσεις μέσω των λέξεων κλειδιά 'serial', 'killer', 'investigate', 'solve', 'case', 'life-threatening', 'homicide', 'inspector'. Το θέμα όλων των ταινιών είναι προφανώς αστυνομικές υποθέσεις με κατα συρροή δολοφονους.   

* id= 3046: Η περίληψη της ταινίας αυτής περιέχει τις λέξεις  "mysterious", "cult", "relegious", "demon", "murder". Οι περιλήψεις των ταινιών που προέκυψαν ως προτάσεις περιέχουν αρκετά συχνά πολλές από τις παραπάνω λέξεις. 

* id=610: Η περιληψη της ταινίας καθώς και οι περιλήψεις των προτάσεων περιέχουν τις λέξεις 'high', 'school', 'teacher', 'coach', 'student'. Το κοινό θέμα όλων των ταινιών είναι ότι διαδρματίζονται σε σχολείο και εμπλεκόμενοι είναι μαθητές και δάσκαλοι. 

* id=1124: Η ταινία αυτή περιέχει τις λέξεις κλειδιά 'space', 'astronaut', 'mankind', 'earth', 'mission', 'launch', 'planet'. Οι προτάσεις περιέχουν επίσης τις παραπάνω λέξεις και το κοινό όλων των ταινιών είναι το διάστημα, και οι διατημικές αποστολές. 


Στα παρακάτω κελιά κώδικα φαίνονται τα αποτελέμστα για είσοδο τα Ids που αναφέρονται παραπάνω
"""

'''
testing with other movies
'''
content_reccomender(2745, 6)

content_reccomender(936, 6)

content_reccomender(6, 6)

content_reccomender(1115, 6)

content_reccomender(4999, 6)

content_reccomender(280, 6)

content_reccomender(340, 6)

content_reccomender(3046, 6)

content_reccomender(610, 6)

content_reccomender(1124, 6)

"""## Συμπεράσματα Πρώτου μέρους

Το σύστημα προτάσεων

# Εφαρμογή 2 - Αυτοοργανομένοι χάρτες (SOM)
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy as sp
from mpl_toolkits.mplot3d import Axes3D
import somoclu

# %matplotlib inline

#from sklearn.decomposition import PCA
#pca = PCA(n_components=2000) 
#corpusTfIdf = pca.fit_transform(corpusTfIdf)

# Commented out IPython magic to ensure Python compatibility.

def build_final_set(doc_limit=5000, tf_idf_only=False):
  if tf_idf_only:
    fin = corpusTfIdf[:doc_limit]
  else:
    fin = np.hstack((corpusTfIdf[:doc_limit], catbins[0:doc_limit,:]))
  return np.array(fin, dtype=np.float32)


def print_categories_stats(ids):
  total_categories = []
  for id in ids:
    curr_categories = np.array2string(categories[id])[2:-2].split(',')
    curr_categories = [cat.strip().strip().strip('"') for cat in curr_categories]
    total_categories.extend(curr_categories)

  cat_names ,cat_count = np.unique(total_categories, return_counts = True)
  cat_sorted = np.argsort(cat_count)[::-1]


  dd = list(zip(cat_names[cat_sorted] , cat_count[cat_sorted]))
  print(dd)
  '''
  for c in cat_sorted:
    print(cat_names[c] + " " + str(cat_count[c]))
  '''


def print_cluster_neurons_movies_report(cluster_id):
  j, i = np.where(clusters == cluster_id)
  idx = np.column_stack((i, j))
  
  total_bmus = []
  for bmu in idx:
    if bmu in ubmus:
        total_bmus.append(bmu)

  return list(total_bmus)


def neurons_movies_report(neurons):
  ind = []
  for i in range(len(bmus)):
    for neuron in neurons:
      if np.array_equal(bmus[i],neuron):
        ind.append(i)

  movies_ids = []
  for i in range(len(indices)):
    if indices[i] in ind:
      movies_ids.append(i)

  print_categories_stats(movies_ids)
  

def run_som_grid_clusters(gridSize, data,saveAsFilename,K_means_clusters=0):
  som = somoclu.Somoclu(gridSize, gridSize ,compactsupport=False)
#   %time som.train(data, epochs=100)
  joblib.dump(som , saveAsFilename)
  if K_means_clusters >0 :
    alg = KMeans(n_clusters=CLUSTERS)
#     %time som.cluster(algorithm=alg)
  return som

"""# Grid Size = 10"""

finalSet =  build_final_set()
print(finalSet.shape)

#som = joblib.load('som10grid.pkl')
som = run_som_grid_clusters(10, finalSet, 'som10grid.pkl')

som.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

"""## Clusters = 10"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
alg = KMeans(n_clusters=10)
# %time som.cluster(algorithm=alg)
som.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som.clusters)

bmus = som.bmus
ubmus, indices = np.unique(bmus, return_inverse=True, axis=0)
print(ubmus.shape)

clusters = som.clusters
uclusters ,counts = np.unique(clusters , return_counts=True)
sorted_counts = np.argsort(counts)
uclusters = uclusters[sorted_counts][::-1]
counts = counts[sorted_counts][::-1]
uclustersDict = dict(zip(uclusters,counts))
print('(cluster, neurons) pairs : ')
print(uclustersDict)

"""Από το παραπάνω λεξικό φαίνεται ότι οι πιο πολλοί νευρώνες μπήκαν στα clusters 3,6,1,7. Βάσει του Umatrix και του πίνακα som.clusters περιμένουμε το 3 και το 6 να είναι σημασιολογικά κοντά. Επίσης, τα 1 και 7 είναι μακρινά μεταξύ τους (σημασιολογικά0 και μακρινά από τα clusters 3, 6."""

neurons_movies_report(print_cluster_neurons_movies_report(3))

neurons_movies_report(print_cluster_neurons_movies_report(1))

neurons_movies_report(print_cluster_neurons_movies_report(6))

neurons_movies_report(print_cluster_neurons_movies_report(7))

"""Παρατηρούμε ότι προβλέψεις μας επαληθεύονται. Παρατηρούμε επίσης ότι εντός ενός cluster υπάρχουν ταινίες διαφορετικών ειδών, κάτι το οποίο δεν είναι καλό, αλλά η κερίαρχη κατηγορία σε κάθε report διαφέρει σημαντικά σε αριθμό από τις επόμενες (τουλάχιστον διαπλάσιο), κάτι το οποίο είναι ικανοποιητικό.

## Clusters = 20
"""

# Commented out IPython magic to ensure Python compatibility.
alg = KMeans(n_clusters=20)
# %time som.cluster(algorithm=alg)
som.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

bmus = som.bmus
ubmus, indices = np.unique(bmus, return_inverse=True, axis=0)
print(ubmus.shape)

clusters = som.clusters
uclusters ,counts = np.unique(clusters , return_counts=True)
sorted_counts = np.argsort(counts)
uclusters = uclusters[sorted_counts][::-1]
counts = counts[sorted_counts][::-1]
uclustersDict = dict(zip(uclusters,counts))
print('(cluster, neurons) pairs : ')
print(uclustersDict)

print(som.clusters)

"""Εδώ, οι πιο πολλοί νευρώνες περιέχονται στα clusters 5, 0, 7 και 1. Από αυτά και με βάση το umatrix και τον πίνακα som.clusters τα 5 και 7 είναι  απομακρυσμένα, τα 5 και 0 είναι αρκετά κοντά και το 1 είναι μακρία από όλα.   """

neurons_movies_report(print_cluster_neurons_movies_report(5))

neurons_movies_report(print_cluster_neurons_movies_report(7))

neurons_movies_report(print_cluster_neurons_movies_report(0))

neurons_movies_report(print_cluster_neurons_movies_report(1))

"""Παρατηρούμε ότι όντως επαληθεύονται οι προβλέψεις που έγιναν παραπάνω, στα clusters 5 και 0 επικρατεί η ίδια κατηγορία (drama) ενώ στα υπόλοιπα clusters (1 και 7) κυριαρχούν οι κατηγορίες Action και Documentary, που είναι μερικώς απομακρισμένες τόσο μεταξύ τους όσο και με τα 5, 0. Και εδώ, ένα cluster περιέχει ταινίες διαφορετικών ειδών, δηλάδή σε μια κλάση υπάρχουν ταινίες της κατηγορίας Drama αλλά και της κατηγορίας Comedy. Αυτό δεν είναι απαγορευτικό, αλλά όταν ο αριθμός των ταινιών της πρώτης κατηγορίας είναι σημαντικά μεγαλύτερος από αυτόν της δεύτερης θεωρούμε ότι η πρώτη κλάση κυριαρχεί και είναι η αντιπροσωπευτική του cluster.

## Clusters = 30
"""

# Commented out IPython magic to ensure Python compatibility.
alg = KMeans(n_clusters=30)
# %time som.cluster(algorithm=alg)
som.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som.clusters)

bmus = som.bmus
ubmus, indices = np.unique(bmus, return_inverse=True, axis=0)
print(ubmus.shape)

clusters = som.clusters
uclusters ,counts = np.unique(clusters , return_counts=True)
sorted_counts = np.argsort(counts)
uclusters = uclusters[sorted_counts][::-1]
counts = counts[sorted_counts][::-1]
uclustersDict = dict(zip(uclusters,counts))
print('(cluster, neurons) pairs : ')
print(uclustersDict)

"""Για τα μεγαλύτερα clusters (3, 11, 1, 16). Βάσει του umatrix και του πίνακα clusters μπορούμε να πούμε με σιγουριά ότι τα clusters 11 και 16 είναι πολύ κοντά σημασιολογικά. """

neurons_movies_report(print_cluster_neurons_movies_report(3))

neurons_movies_report(print_cluster_neurons_movies_report(11))

neurons_movies_report(print_cluster_neurons_movies_report(1))

neurons_movies_report(print_cluster_neurons_movies_report(16))

neurons_movies_report(print_cluster_neurons_movies_report(13))

"""Σε αυτή την περίτωση αξίζει να πούμε ότι ο διαχωρισμός πέτυχε πολύ καλά στην περίπτωση του cluster 13. Εκεί, οι δύο πρώτες κατηγορίες είχαν περίπου περίπου ίδιο αριθμό εμφανίσεων και άρα δεν μπορούσαμε να θεωρήσουμε την πρώτη ως αντιπροσωπευτική. Ωστόσο, οι δύο αυτές πρώτες κατηγορίες είναι μεταξύ τους παρόμοιες (horror και slasher).

# Grid Size = 20
"""

som = run_som_grid_clusters(20, finalSet, 'som20grid.pkl')
#som = joblib.load('som20grid.pkl')

"""## Clusters = 15"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
alg = KMeans(n_clusters=15)
# %time som.cluster(algorithm=alg)
som.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som.clusters)

bmus = som.bmus
ubmus, indices = np.unique(bmus, return_inverse=True, axis=0)
print(ubmus.shape)

clusters = som.clusters
uclusters ,counts = np.unique(clusters , return_counts=True)
sorted_counts = np.argsort(counts , )
uclusters = uclusters[sorted_counts][::-1]
counts = counts[sorted_counts][::-1]
uclustersDict = dict(zip(uclusters,counts))
print('(cluster, neurons) pairs : ')
print(uclustersDict)

"""Από τα παραπάνω Clusters, θα εξετάσουμε τα 11, 9 και 7, τα οποία βάσει των πινάκων umatrix και clusters είναι απομακρυσμένα μεταξύ τους.




"""

neurons_movies_report(print_cluster_neurons_movies_report(11))

neurons_movies_report(print_cluster_neurons_movies_report(9))

neurons_movies_report(print_cluster_neurons_movies_report(7))

"""Πράγματι, τα 11 και 7 είναι μεταξύ τους πολύ μακρινά (drama και comedy) ενώ το 9 είναι απομακρυσμένο αλλά πιο κοντά στο 11 από ότι στο 7

## Clusters = 20
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
alg = KMeans(n_clusters=20)
# %time som.cluster(algorithm=alg)
som.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som.clusters)

clusters = som.clusters
uclusters ,counts = np.unique(clusters , return_counts=True)
sorted_counts = np.argsort(counts , )
uclusters = uclusters[sorted_counts][::-1]
counts = counts[sorted_counts][::-1]
uclustersDict = dict(zip(uclusters,counts))
print('(cluster, neurons) pairs : ')
print(uclustersDict)

bmus = som.bmus
ubmus, indices = np.unique(bmus, return_inverse=True, axis=0)
print(ubmus.shape)

"""Για την περίπτωση αυτή δοκιμάζουμε τις κλάσεις 4, 0, 7, 18 που βρίσκονται όλες μακριά η μια από την άλλη σύμφωνα με τον πίνακα cluster και umatrix."""

neurons_movies_report(print_cluster_neurons_movies_report(4))

neurons_movies_report(print_cluster_neurons_movies_report(0))

neurons_movies_report(print_cluster_neurons_movies_report(7))

neurons_movies_report(print_cluster_neurons_movies_report(18))

"""Πράγματι, όλες οι ταινίες είναι απέχουν σημαντικά μεταξύ τους ως προς την πρώτη τους κατηγορία. Παρατηρούμε ωστόσο ότι όλες οι μεγάλες κατηγορίες κυριαρχούνται από την κατηγορία drama και αν εξετάσυμε τα clusters με τους πιο πολλούς νευρώνες θα δούμε ότι η διάταξη είναι χειρότερη από αυτήν που είχαμε για 15 clusters. Αυτό ίσως σημαίνει ότι τα 20 clusters είναι πολλά και γι αυτό η επόμενη τιμή θα είναι 17 clusters.

## Clusters = 17
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
alg = KMeans(n_clusters=17)
# %time som.cluster(algorithm=alg)
som.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som.clusters)

clusters = som.clusters
uclusters ,counts = np.unique(clusters , return_counts=True)
sorted_counts = np.argsort(counts , )
uclusters = uclusters[sorted_counts][::-1]
counts = counts[sorted_counts][::-1]
uclustersDict = dict(zip(uclusters,counts))
print('(cluster, neurons) pairs : ')
print(uclustersDict)

bmus = som.bmus
ubmus, indices = np.unique(bmus, return_inverse=True, axis=0)
print(ubmus.shape)

neurons_movies_report(print_cluster_neurons_movies_report(15))

neurons_movies_report(print_cluster_neurons_movies_report(16))

neurons_movies_report(print_cluster_neurons_movies_report(11))

neurons_movies_report(print_cluster_neurons_movies_report(8))

neurons_movies_report(print_cluster_neurons_movies_report(6))

"""Παρατηρούμε πάλι ότι η διάταξη είναι χειρότερη από αυτή που είχαμε για 15 clusters οπότε θα αυξήσουμε το μέγεθος του grid για να συνεχίστεί η διερεύνηση.

# Grid Size = 30
"""

som = run_som_grid_clusters(30, finalSet, 'som30grid.pkl')
#som = joblib.load('som30grid.pkl')

"""## Clusters = 18"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
alg = KMeans(n_clusters=18)
# %time som.cluster(algorithm=alg)
som.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

with open('cluster30-grid18.txt', 'w+') as f:
  for x in som.clusters:
    for y in x:
      if y >= 10:
        f.write(str(y) + ' ')
      else:
        f.write('0' + str(y) + ' ')
    f.write('\n')

clusters = som.clusters
uclusters ,counts = np.unique(clusters , return_counts=True)
sorted_counts = np.argsort(counts , )
uclusters = uclusters[sorted_counts][::-1]
counts = counts[sorted_counts][::-1]
uclustersDict = dict(zip(uclusters,counts))
print('(cluster, neurons) pairs : ')
print(uclustersDict)

bmus = som.bmus
ubmus, indices = np.unique(bmus, return_inverse=True, axis=0)
print(ubmus.shape)

neurons_movies_report(print_cluster_neurons_movies_report(9))

neurons_movies_report(print_cluster_neurons_movies_report(14))

neurons_movies_report(print_cluster_neurons_movies_report(8))

neurons_movies_report(print_cluster_neurons_movies_report(7))

neurons_movies_report(print_cluster_neurons_movies_report(13))

"""Η διάταξη σε αυτό το χάρτη δεν έχει συνοχή ως προς την πρώτη κατηγορία. Η συνολική κατηγοριοποίηση είναι χειρότερη από το αποτέλεσμα που προέκυψε για grid size = 20 και 15 clusters.

## Clusters = 30
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
alg = KMeans(n_clusters=30)
# %time som.cluster(algorithm=alg)
som.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som.clusters)
with open('cluster10-grid30.txt', 'w+') as f:
  for x in som.clusters:
    for y in x:
      if y >= 10:
        f.write(str(y) + ' ')
      else:
        f.write('0' + str(y) + ' ')
    f.write('\n')

clusters = som.clusters
uclusters ,counts = np.unique(clusters , return_counts=True)
sorted_counts = np.argsort(counts , )
uclusters = uclusters[sorted_counts][::-1]
counts = counts[sorted_counts][::-1]
uclustersDict = dict(zip(uclusters,counts))
print('(cluster, neurons) pairs : ')
print(uclustersDict)

bmus = som.bmus
ubmus, indices = np.unique(bmus, return_inverse=True, axis=0)
print(ubmus.shape)

"""Εδώ, οι πιο πολλοί νευρώνες συγκεντρώνονται στα clusters 8, 11, 24, 18, 10. Θα δοκιμαστούν τα αποτελέσματα των clusters 8, 24 και 16."""

neurons_movies_report(print_cluster_neurons_movies_report(8))

neurons_movies_report(print_cluster_neurons_movies_report(24))

neurons_movies_report(print_cluster_neurons_movies_report(16))

"""και εδώ ο χάρτης δεν είναι τόσο καλός. Τα clusters 8 και 24 είναι σε δύο άκρα του χάρτη άρα θα  έπρεπε να είναι σημασιολογικά μακρινά. Παρόλ αυτά όμως έχουν ίδιες κατηγορίες ταινιών.

## Συμπεράσματα δεύτερου μέρους

Από τους συνδυασμούς που δοκιμάστηκαν στο μέρος 2 αυτός που έδωσε τα πιο ικανοποιητικά αποτελέσματα είναι το μέγεθος πλέγματος 20 με 15 κατηγορίες. Στο grid με μέγεθος πλευράς 30 τα αποτελέσματα δεν είχαν διάταξη. 

<br>

Ενδεχωμένως να υπάρχει κάποιο καλύτερο μέγεθος πλέγματος στο διάστημα (20, 30). Όσο αυξάνεται ο αριθμός του πλέγματος δεν σημαίνει απαραίτητα ότι η ποιότητα βελτιώνεται και πολλοί νευρώνες μένουν αχρησιμοποίητοι κάτι το οποίο ίσως υποδηλώμει κορεσμό. 

<br>

Ο χάρτης som που κατασκυάστηκε εν τέλει χρειάζεται πολλές βελτιώσεις και ίσως να κουβαλάει και λάθη από το μέρος ένα (tfidf vectorizer). Ωστόσο, σε κάποια συγκεκριμένα clusters δίνει καλά αποτελέσματα και αυτό είναι το ενθαρυντικό. 

<br>

Ένα πρόβλημα το οποίο παρατηρήθηκε είναι ότι η συντριπτική πλειοψηφία των ταινιών περιείχε σαν κατηγορία το 'drama'. Για το λόγο αυτό σχεδόν σε όλα τα clusters υπήρχε και το drαma και μάλιστα με μεγάλη συχνότητα. Θα μπορούσαμε να εφαρμόσουμε κάποια τεχνική όπως oversampling για να σταθμίσουμε τις κατηγορίες αλλά αυτό θα δημιουργούσε πολλά νέα δείγματα και θα εκτόξευε τους χρόνους εκπαίδευσης.
"""